# ğŸ” Loghub Detection â€“ Lambda Architecture

<p align="center">
  <img src="https://img.shields.io/badge/BigData-Hadoop-blue?style=flat-square" />
  <img src="https://img.shields.io/badge/Streaming-Kafka-orange?style=flat-square" />
  <img src="https://img.shields.io/badge/Storage-HBase-green?style=flat-square" />
  <img src="https://img.shields.io/badge/Compute-Spark-red?style=flat-square" />
  <img src="https://img.shields.io/badge/Architecture-Lambda-purple?style=flat-square" />
</p>

Há»‡ thá»‘ng phÃ¢n tÃ­ch log HDFS theo thá»i gian thá»±c & batch.

---

# ğŸ“Œ Giá»›i thiá»‡u

Loghub Detection lÃ  há»‡ thá»‘ng phÃ¡t hiá»‡n vÃ  phÃ¢n tÃ­ch log HDFS dá»±a trÃªn kiáº¿n trÃºc **Lambda Architecture**, káº¿t há»£p cáº£:

- **Batch Layer** â€“ xá»­ lÃ½ dá»¯ liá»‡u log lá»‹ch sá»­ (file Loghub lá»›n).
- **Speed Layer (Streaming)** â€“ xá»­ lÃ½ log má»›i theo thá»i gian thá»±c qua Kafka.
- **Serving Layer** â€“ cung cáº¥p káº¿t quáº£ há»£p nháº¥t cho API/UI.

Má»¥c tiÃªu há»‡ thá»‘ng:

- Chuáº©n hÃ³a log HDFS (block, timestamp, duration...).
- PhÃ¡t hiá»‡n báº¥t thÆ°á»ng (anomaly) hoáº·c lá»—i block.
- LÆ°u trá»¯ phÃ¢n tÃ¡n vá»›i HBase.
- GiÃ¡m sÃ¡t realtime qua Kafka Producer/Consumer.

---

# ğŸ§± Kiáº¿n trÃºc tá»•ng quan

![Lambda](./lambda.drawio.png)

---

# ğŸ—‚ï¸ Cáº¥u trÃºc thÆ° má»¥c project

```
BIG-DATA/
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ loghub-detection/
â”‚           â”œâ”€â”€ __pycache__/
â”‚           â”œâ”€â”€ static/
â”‚           â”œâ”€â”€ templates/
â”‚           â”œâ”€â”€ app.py
â”‚           â”œâ”€â”€ config.py
â”‚           â”œâ”€â”€ docker-compose.yaml
â”‚           â”œâ”€â”€ Dockerfile
â”‚           â”œâ”€â”€ get_data_from_hbase.py
â”‚           â”œâ”€â”€ hbase_client.py
â”‚           â””â”€â”€ Makefile
â”‚
â”œâ”€â”€ batch_layer/
â”‚   â””â”€â”€ batch_analysis/
â”‚       â”œâ”€â”€ base/
â”‚       â”œâ”€â”€ data/
â”‚       â”œâ”€â”€ datanode/
â”‚       â”œâ”€â”€ hdfs/
â”‚       â”œâ”€â”€ historyserver/
â”‚       â”œâ”€â”€ namenode/
â”‚       â”œâ”€â”€ nginx/
â”‚       â”œâ”€â”€ nodemanager/
â”‚       â”œâ”€â”€ notebook/
â”‚       â”œâ”€â”€ resourcemanager/
â”‚       â”œâ”€â”€ docker-compose.yml
â”‚       â”œâ”€â”€ hadoop.env
â”‚       â”œâ”€â”€ Makefile
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ Power_BI/
â”‚
â”œâ”€â”€ load_data_to_sql.py
â”‚
â”œâ”€â”€ data/
â”‚
â”œâ”€â”€ model_ML/
â”‚
â”œâ”€â”€ stream_layer/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ HDFS.log_templates.csv
â”‚   â”œâ”€â”€ insert_data_to_HBase.py
â”‚   â”œâ”€â”€ kafka_consumer.py
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”œâ”€â”€ ML_predict.py
â”‚   â”œâ”€â”€ pipeline_stream.py
â”‚   â”œâ”€â”€ stream_data.py
â”‚   â””â”€â”€ transform.py
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ ip_localhost.txt
â”œâ”€â”€ lambda.drawio.png
â”œâ”€â”€ README.md
â””â”€â”€ spec.txt

```

---

# âš™ï¸ Batch Layer â€“ Xá»­ lÃ½ dá»¯ liá»‡u log lá»‹ch sá»­

### ğŸ“¥ Input

- File log tá»« Loghub HDFS dáº¡ng `.log` .

### ğŸ§¹ CÃ¡c bÆ°á»›c xá»­ lÃ½ báº±ng Spark

- TÃ¡ch `BlockId` (`blk_...`).
- TrÃ­ch xuáº¥t `start_ts`, `end_ts`, tÃ­nh `duration_sec`.
- Gom log theo tá»«ng block thÃ nh **block session**.
- TÃ­nh tá»•ng sá»‘ dÃ²ng log (`num_lines`).
- Chuáº©n hÃ³a dá»¯ liá»‡u vá» dáº¡ng **row-based** phá»¥c vá»¥ phÃ¢n tÃ­ch vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh.

### ğŸ“¤ Output

```
blockId | start_ts | end_ts | duration_sec | log_full | num_lines
```

LÆ°u vÃ o:

- HDFS vÃ   `batch_layer/batch_layer/data`
- PosterSQL

---

# âš¡ Speed Layer â€“ Xá»­ lÃ½ realtime báº±ng Kafka

### 1) Log Generator

Sinh log mÃ´ phá»ng realtime:

```json
{
  "blockId": "...",
  "start_ts": "...",
  "end_ts": "...",
  "duration_sec": 0,
  "num_lines": 2,
  "log_full": "raw text..."
}
```

### 2) Kafka Producer

Gá»­i JSON message vÃ o topic:

```
topic = loghub-realtime
```

### 3) Kafka Consumer

Nháº­n log â†’ Transformâ†’ Predict â†’ LÆ°u vÃ o HBase.

### 4) HBase Storage Schema

```
table: loghub
column family: info
columns:
  - start_ts
  - end_ts
  - duration
  - log_full
  - num_lines
```

Má»—i block lÃ  má»™t row-key.

---

# ğŸ¦ Serving Layer

Serving Layer káº¿t há»£p dá»¯ liá»‡u tá»«:

| Nguá»“n               | Chá»©c nÄƒng                         |
| -------------------- | ----------------------------------- |
| **Batch View** | dá»¯ liá»‡u lá»‹ch sá»­, Ä‘áº§y Ä‘á»§     |
| **Speed View** | dá»¯ liá»‡u realtime (Kafka â†’ HBase) |

Táº¥t cáº£ Ä‘Æ°á»£c lÆ°u trong HBase vÃ  query qua API/UI.

---

# ğŸš€ CÃ¡ch cháº¡y project

### 1) Cháº¡y Batch Layer

Khá»Ÿi Ä‘á»™ng Docker

```bash
cd jobs/batch_layer/batch_analysis
docker-compose up -d
```

ThÃªm dá»¯ liá»‡u vÃ o HDFS

```bash
hdfs dfs -mkdir -p /user/root/data/tmp/datack/
hdfs dfs -put HDFS.log /user/root/data/tmp/datack/
```

Má»Ÿ container Jupyter NoteBook táº¡i Ä‘á»‹a chá»‰ : localhost:8888

LÆ°u dá»¯ liá»‡u vÃ o PosterSQL

```bash
python load_data_to_sql.py
```

### 2) Stream Layer

Khá»Ÿi Ä‘á»™ng Docker

```bash
cd jobs/stream_layer
docker compose up -d
```

Cháº¡y mÃ´ phÃ²ng stream

```bash
python pipeline_stream.py
```

### 3) Hiá»ƒn thá»‹ giao diá»‡n

```bash
cd jobs/app/loghub_detection
python app.py
```

---

# ğŸ§ª Káº¿t quáº£ demo

![Lambda](./demo.png)

![Power-BI](./dashboard.png "Power-BI")

---

# ğŸ“ˆ HÆ°á»›ng phÃ¡t triá»ƒn

- ThÃªm module anomaly detection.
- DÃ¹ng PySpark Streaming thay Python Consumer.
- Tá»‘i Æ°u storage báº±ng HBase compression.
- Dashboard realtime báº±ng Grafana.

---

# ğŸ“š TÃ i liá»‡u tham kháº£o

- [Loghub Dataset](https://github.com/logpai/loghub)
- [Apache Kafka](https://kafka.apache.org/)
- [Apache HBase](https://hbase.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [Lambda Architecture (Batch + Speed)](https://lambda-architecture.net/)

---


# ğŸ‘¥ ThÃ nh viÃªn nhÃ³m

| Há» vÃ  tÃªn                    | MÃ£ sinh viÃªn |
| ------------------------------- | -------------- |
| **Nguyá»…n VÄƒn Viá»‡t**    | 23020430       |
| **NgÃ´ Äinh Minh Nháº­t** | 23020408       |
| **Kiá»u Äá»©c Nam**       | 23020404       |
